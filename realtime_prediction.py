import cv2
import numpy as np
import threading
import time
import tkinter as tk
from tkinter import Button
from PIL import Image, ImageTk
from keras.models import load_model
from gtts import gTTS
import pygame
import os
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
import tensorflow as tf
import tempfile

# Khá»Ÿi táº¡o pygame cho Ã¢m thanh
pygame.mixer.init()

# Load mÃ´ hÃ¬nh
model = load_model("C:/Users/DELL/Desktop/code/models/emotion_cnn.h5")

# Danh sÃ¡ch nhÃ£n biá»ƒu cáº£m vÃ  cÃ¢u nÃ³i tiáº¿ng Viá»‡t
emotion_labels = ['Angry', 'Fear', 'Happy', 'Sad', 'Surprise']
emotion_speech = {
    'Angry': "Äá»«ng giáº­n dá»¯ nhÆ° váº­y mÃ . HÃ£y ngá»“i xuá»‘ng vÃ  hÃ­t thá»Ÿ tháº­t sÃ¢u!",
    'Fear': "Báº¡n Ä‘ang sá»£ Ä‘iá»u gÃ¬ Ã ?",
    'Happy': "CÃ³ váº» báº¡n cÃ³ má»™t ngÃ y tuyá»‡t vá»i nhá»‰. HÃ£y lan tá»a nÃ³ tá»›i má»i ngÆ°á»i nÃ o!",
    'Sad': "Äá»«ng buá»“n nhÃ©. CÃ³ mÃ¬nh Ä‘Ã¢y rá»“i.",
    'Surprise': "á»’, cÃ³ gÃ¬ lÃ m báº¡n ngáº¡c nhiÃªn váº­y?",
}

# Tráº¡ng thÃ¡i Ã¢m thanh
audio_enabled = True
last_speak_time = 0
speak_interval = 5

def play_audio(text):
    temp_fd, temp_path = tempfile.mkstemp(suffix=".mp3")
    os.close(temp_fd)  # ÄÃ³ng file descriptor Ä‘á»ƒ trÃ¡nh lá»—i trÃªn Windows
    try:
        tts = gTTS(text=text, lang='vi')
        tts.save(temp_path)
        pygame.mixer.music.load(temp_path)
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy():
            time.sleep(0.1)
    finally:
        os.remove(temp_path)  # XÃ³a file sau khi phÃ¡t xong


# HÃ m phÃ¡t giá»ng nÃ³i báº±ng luá»“ng riÃªng Ä‘á»ƒ trÃ¡nh lag
def speak(text):
    if audio_enabled:
        threading.Thread(target=play_audio, args=(text,), daemon=True).start()

# HÃ m dá»± Ä‘oÃ¡n biá»ƒu cáº£m
def predict_emotion(face_image):
    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
    face_image = cv2.resize(face_image, (48, 48))
    face_image = face_image / 255.0
    face_image = np.expand_dims(face_image, axis=(0, -1))
    predictions = model.predict(face_image)
    return emotion_labels[np.argmax(predictions)]

# HÃ m cáº­p nháº­t video
def update_frame():
    global last_speak_time
    ret, frame = cap.read()
    if not ret:
        return

    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(48, 48))
    current_time = time.time()

    for (x, y, w, h) in faces:
        face = frame[y:y+h, x:x+w]
        emotion = predict_emotion(face)

        if emotion in emotion_speech and (current_time - last_speak_time >= speak_interval):
            speak(emotion_speech[emotion])
            last_speak_time = current_time

        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)
        cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)

    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    img = Image.fromarray(frame)
    img_tk = ImageTk.PhotoImage(image=img)
    canvas.create_image(0, 0, anchor=tk.NW, image=img_tk)
    canvas.image = img_tk
    root.after(10, update_frame)

# HÃ m báº­t/táº¯t Ã¢m thanh
def toggle_audio():
    global audio_enabled
    audio_enabled = not audio_enabled
    audio_button.config(text="ðŸ”Š ON" if audio_enabled else "ðŸ”‡ OFF")

# Táº¡o cá»­a sá»• Tkinter
root = tk.Tk()
root.title("Emotion Recognition")

canvas = tk.Canvas(root, width=640, height=480)
canvas.pack()

audio_button = Button(root, text="ðŸ”Š ON", command=toggle_audio, font=("Arial", 12))
audio_button.pack()

# Má»Ÿ webcam
cap = cv2.VideoCapture(0)
update_frame()

root.mainloop()

cap.release()
cv2.destroyAllWindows()
